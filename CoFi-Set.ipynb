{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport csv\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class COFISET:\n    \"\"\"\n    COFISET which predicts unknown ratings of a movie to any user..\n    \n    Parameters:\n    --------------\n    class_boundary : int\n        items with ratings [1-class_boudary] considered as unobserved item set I_tr/I_u_tr\n        remaining rating items considered as observed set\n    p_size : int\n        size of set P (Observed Item subset)\n    a_size : int\n        size of set A (UnObserved Item subset)\n    n_factors : int\n        Number of latent factors\n    gamma : float\n        learning rate\n    alpha_u : float\n        regularisation constant ,hyperparameter\n    alpha_v : float\n        regularisation constant ,hyperparameter\n    beta_v : float\n        regularisation constant ,hyperparameter\n    \n    \"\"\"\n    \n    def __init__(self,class_boundary = 3,p_size = 4,a_size = 2,n_factors = 10,gamma = 0.01,alpha_u = 0.01 ,alpha_v = 0.01,beta_v = 0.01):\n        self.class_boundary = class_boundary\n        self.p_size = p_size\n        self.a_size = a_size\n        self.n_factors = n_factors\n        self.n_users = 943          #fixed for our project's data set - movielens ml-100k\n        self.n_items = 1682\n        self.gamma = gamma\n        self.alpha_u = alpha_u\n        self.alpha_v = alpha_v\n        self.beta_v = beta_v\n       \n        # initialisation of pu(user latent factor) matrix , qi(item latent factor) matrix and bi(item bias) vector.....\n        r = random.random() # random number r in [0,1)\n        self.pu = (np.random.rand(self.n_users,self.n_factors) - 0.5)/150\n        self.qi = (np.random.rand(self.n_items,self.n_factors)  - 0.5)/150     \n                \n        \n    def fit(self,filename,T = 10**4):\n        self.observed_item_set,self.training_item_set,self.bi = self.load_data(filename)\n        #from training data set ....\n        \n        #initialisation of item bias vector\n        self.bi /= self.n_users   \n        self.bi -= self.bi.mean()\n        \n        for t1 in range(0,self.n_users): # put T in 1\n            for t2 in range(0,T): #Put self.n_users in 1\n                u = random.randrange(0,self.n_users) # randomly choose an user\n\n                #-----exceptions handling......\n\n                if not(u in self.observed_item_set.keys()):\n                    continue;\n\n                if(self.p_size > len(self.observed_item_set[u]) or self.a_size > len(self.training_item_set - self.observed_item_set[u])):\n                    continue; # P and A sizes must be less than existing items...\n                                # (training_item_set - observed_item_set[u]) is unobserved itemset for that user....\n                #-- random sampling of observed and unobserved item subsets for a user\n                A = random.sample(self.training_item_set - self.observed_item_set[u],self.a_size)\n                P = random.sample(self.observed_item_set[u],self.p_size)\n\n                # --- Gradient Descent Strarts here................\n\n                    #---- For observed Item Subset P\n                Vp = np.zeros(self.n_factors,float) # row vector with all 0's\n                Bp = 0                  #total bias of observed set\n\n                for i in P:\n                    Vp = Vp + self.qi[i,:]\n                    Bp = Bp + self.bi[i]\n                \n                rating_u_p = self.pu[u,:].dot(Vp) + Bp # sum of all predicted ratings for observed set\n                rating_u_p_avg = rating_u_p/self.p_size    # average of observed set r_u_p\n\n                    #----- For unobserved Item Subset A\n                Va = np.zeros(self.n_factors,float) # row vector with all 0's\n                Ba = 0                  #total bias of unobserved set\n\n                for j in A:\n                    Va = Va + self.qi[j,:]\n                    Ba = Ba + self.bi[j]\n\n                rating_u_a = self.pu[u,:].dot(Va) + Ba\n                rating_u_a_avg = rating_u_a/self.a_size\n\n                    #---- loss function....\n                loss = -1/(1+np.exp(rating_u_p_avg - rating_u_a_avg)) # -sigma(-R_{u,P,A})\n\n                Vp_avg = Vp/self.p_size\n                Va_avg = Va/self.a_size\n\n                    # ---- update latent vectors of observed items.....\n                for i in P:         # do it before updation of user latent vector...\n                    self.qi[i,:] -= self.gamma * (loss/self.p_size * self.pu[u,:] + self.alpha_v*self.qi[i,:])\n                    self.bi[i] -= self.gamma * (loss/self.p_size + self.beta_v * self.bi[i])\n\n                for j in A:\n                    self.qi[j,:] -= self.gamma * (-loss/self.a_size * self.pu[u,:] + self.alpha_v*self.qi[j,:])\n                    self.bi[j] -= self.gamma * (-loss/self.a_size + self.beta_v * self.bi[j]) \n\n                self.pu[u,:] -= self.gamma * (loss * (Vp_avg - Va_avg) + self.alpha_u * self.pu[u,:])\n            print(\"Epoch - {}\".format(t1+1),end=\" \")\n    \n    # Transform user ids and item ids by substracting 1 from it .....\n\n    def load_data(self,filename): # returns observed and unobserved item sets for each user\n        observed_item_set = dict() # observed item set for each user... I_u_tr or I_u_te\n        total_item_set = set()  # Set of items that are observed by any user... I_tr or I_te\n                                # to create empty set, use set() function\n        bi = np.zeros(self.n_items,float)\n                                # creating bias vector for all items\n        with open(filename) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n            # each row is in format of user_id,item_id,rating,timestamp\n            for row in csv_reader:\n                row[0] = int(row[0])-1 ;row[1] = int(row[1])-1;row[2] = int(row[2])\n                if(row[2] > self.class_boundary): #keep it in observed set dictionary\n                    bi[row[1]] += 1    # increment count of training user w.r.t item i....\n                    if(row[0] in observed_item_set.keys()): # user is already present add this item id \n                        observed_item_set[row[0]].add(row[1])\n                    else:  #user is not present,create entry for that user in observed set\n                        observed_item_set[row[0]] = {row[1]}  #creating new set for a user with item id\n                        \n                    total_item_set.add(row[1])\n                 \n        return observed_item_set,total_item_set,bi\n        \ndef evaluate(self,filename):\n        # all these are testing related data ...\n        predicted = self.pu.dot(self.qi.transpose()) + self.bi\n        \n        test_observed,test_item_set,junk = self.load_data(filename)\n        test_user_count = len(test_observed.keys())\n        # self.observed_item_set,self.training_item_set are related to training data.....\n        precision = 0 \n        NDCG = 0 ; max_dcg = 1;\n        MRR = 0\n        ARP = 0\n        AUC = 0\n        for user in test_observed.keys():\n            dcg = 0;rr = 0;rp = 0;auc = 0;\n            predicted_user_ratings = predicted[user,:]\n            predicted_user_ratings[list(self.observed_item_set[user])] = 0 \n                # making training observed item rating as 0\n            rank_list = list(np.argsort(predicted_user_ratings)[-1::-1])   # ranking of items by their scores.. It stores item indices in ascending\n           \n            for l in range(0,5): # Taking top 5 recommendations... \n                if rank_list[l] in test_observed[user]:\n                    precision += 1\n                    dcg += 1/np.log2(l+2)  # what is the base... l strarts from 0 here\n                    \n            NDCG += dcg\n            if(dcg > max_dcg):\n                max_dcg= dcg    \n            \n            test_observed_count = 0\n            rank = 1\n            for item in rank_list:\n                if(item in test_observed[user]):\n                    test_observed_count  += 1\n                    rp += rank\n                    if(rr == 0):  # take first test set item's position for a user\n                        rr = rank\n\n                elif(predicted[user,item] != 0):\n                    auc +=test_observed_count;\n                rank +=1\n                \n            MRR += (1/rr)    \n            AUC += auc/(len(test_observed[user]) * (1682-len(test_observed[user])-len(self.observed_item_set[user])))\n            ARP =+ rp/(len(test_observed[user]) * (len(self.training_item_set) - len(self.observed_item_set[user])))\n\n        precision /= (5*test_user_count)\n        NDCG /= (max_dcg*test_user_count)\n        MRR /= test_user_count\n        ARP /= test_user_count\n        AUC /= test_user_count\n        print(\"d \\talpha_u | alpha_v | beta_v |   Pre@5   |   NDCG@5|    MRR    |   ARP   |   AUC   |\")\n        print(\"COFISET {}\\t| {}\\t  | {}   |  {:.4f}   |   {:.4f}|  {:.4f}   |  {:.4f} |  {:.4f} |\".format(self.alpha_u,self.alpha_v,\n                                                                                            self.beta_v,precision,NDCG,MRR,ARP,AUC))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncofiset = COFISET()\ncofiset.fit(r\"/kaggle/input/u.base\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def evaluate(self,filename):\n        # all these are testing related data ...\n        predicted = self.pu.dot(self.qi.transpose()) + self.bi\n        \n        test_observed,test_item_set,junk = self.load_data(filename)\n        test_user_count = len(test_observed.keys())\n        # self.observed_item_set,self.training_item_set are related to training data.....\n        precision = 0 \n        NDCG = 0 ; max_dcg = 1;\n        MRR = 0\n        ARP = 0\n        AUC = 0\n        for user in test_observed.keys():\n            dcg = 0;rr = 0;rp = 0;auc = 0;\n            predicted_user_ratings = predicted[user,:]\n            predicted_user_ratings[list(self.observed_item_set[user])] = 0 \n                # making training observed item rating as 0\n            rank_list = list(np.argsort(predicted_user_ratings)[-1::-1])   # ranking of items by their scores.. It stores item indices in ascending\n           \n            for l in range(0,5): # Taking top 5 recommendations... \n                if rank_list[l] in test_observed[user]:\n                    precision += 1\n                    dcg += 1/np.log2(l+2)  # what is the base... l strarts from 0 here\n                    \n            NDCG += dcg\n            if(dcg > max_dcg):\n                max_dcg= dcg    \n            \n            test_observed_count = 0\n            rank = 1\n            for item in rank_list:\n                if(item in test_observed[user]):\n                    test_observed_count  += 1\n                    rp += rank\n                    if(rr == 0):  # take first test set item's position for a user\n                        rr = rank\n\n                elif(predicted[user,item] != 0):\n                    auc +=test_observed_count;\n                rank +=1\n                \n            MRR += (1/rr)    \n            AUC += auc/(len(test_observed[user]) * (1682-len(test_observed[user])-len(self.observed_item_set[user])))\n            ARP += rp/(len(test_observed[user]) * (len(self.training_item_set) - len(self.observed_item_set[user])))\n\n        precision /= (5*test_user_count)\n        NDCG /= (max_dcg*test_user_count)\n        MRR /= test_user_count\n        ARP /= test_user_count\n        AUC /= test_user_count\n        print(\"d \\talpha_u | alpha_v | beta_v |   Pre@5   |   NDCG@5|    MRR    |   ARP   |   AUC   |\")\n        print(\"COFISET {}\\t| {}\\t  | {}   |  {:.4f}   |   {:.4f}|  {:.4f}   |  {:.4f} |  {:.4f} |\".format(self.alpha_u,self.alpha_v,\n                                                                                            self.beta_v,precision,NDCG,MRR,ARP,AUC))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nevaluate(cofiset,r\"/kaggle/input/u.test\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}